{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "The end result of this exercise should be a file named prepare.py that defines the requested functions.\n",
    "\n",
    "In this exercise we will be defining some functions to prepare textual data. These functions should apply equally well to both the codeup blog articles and the news articles that were previously acquired.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function named basic_clean. It should take in a string and apply some basic text cleaning to it:\n",
    "# - Lowercase everything\n",
    "# - Normalize unicode characters\n",
    "# - Replace anything that is not a letter, number, whitespace or a single quote.\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def basic_clean(text):\n",
    "    \"\"\"\n",
    "    This function takes in a string and applies some basic text cleaning to it:\n",
    "    - Lowercase everything\n",
    "    - Normalize unicode characters\n",
    "    - Replace anything that is not a letter, number, whitespace or a single quote.\n",
    "    \"\"\"\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Normalize unicode characters\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    \n",
    "    # Replace anything that is not a letter, number, whitespace or a single quote\n",
    "    text = re.sub(r\"[^a-z0-9'\\s]\", '', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function named tokenize. It should take in a string and tokenize all the words in the string.\n",
    "\n",
    "import nltk\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    This function takes in a string and tokenize all the words in the string.\n",
    "    \"\"\"\n",
    "    # Create the tokenizer\n",
    "    tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "    \n",
    "    # Use the tokenizer\n",
    "    text = tokenizer.tokenize(text, return_str=True)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function named stem. It should accept some text and return the text after applying stemming to all the words.\n",
    "\n",
    "def stem(text):\n",
    "    \"\"\"\n",
    "    This function takes in some text and return the text after applying stemming to all the words.\n",
    "    \"\"\"\n",
    "    # Create the nltk stemmer object, then use it\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    stems = [ps.stem(word) for word in text.split()]\n",
    "    \n",
    "    # Join our lists of words into a string again\n",
    "    text_stemmed = ' '.join(stems)\n",
    "    \n",
    "    return text_stemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 4. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/zacschmitz/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "# Define a function named lemmatize. It should accept some text and return the text after applying lemmatization to each word.\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def lemmatize(text):\n",
    "    \"\"\"\n",
    "    This function takes in some text and return the text after applying lemmatization to each word.\n",
    "    \"\"\"\n",
    "    # Create the nltk lemmatizer object, then use it\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    lemmas = [wnl.lemmatize(word) for word in text.split()]\n",
    "    \n",
    "    # Join our list of words into a string again\n",
    "    text_lemmatized = ' '.join(lemmas)\n",
    "    \n",
    "    return text_lemmatized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 5. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/zacschmitz/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Define a function named remove_stopwords. It should accept some text and return the text after removing all the stopwords.\n",
    "# This function should define two optional parameters, extra_words and exclude_words. These parameters should define any additional stop words to include, and any words that we don't want to remove.\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def remove_stopwords(text, extra_words=None, exclude_words=None):\n",
    "    \"\"\"\n",
    "    This function takes in some text and returns the text after removing all the stopwords.\n",
    "    It defines two optional parameters, extra_words and exclude_words, which define any additional stop words to include,\n",
    "    and any words that we don't want to remove.\n",
    "    \"\"\"\n",
    "    # Get the list of stopwords\n",
    "    stopword_list = stopwords.words('english')\n",
    "    \n",
    "    # Add any extra words to the stopword list\n",
    "    if extra_words:\n",
    "        stopword_list += extra_words\n",
    "    \n",
    "    # Remove any exclude words from the stopword list\n",
    "    if exclude_words:\n",
    "        stopword_list = [word for word in stopword_list if word not in exclude_words]\n",
    "    \n",
    "    # Tokenize the text\n",
    "    words = text.split()\n",
    "    \n",
    "    # Remove the stopwords from the text\n",
    "    filtered_words = [word for word in words if word not in stopword_list]\n",
    "    \n",
    "    # Join the filtered words back into a string\n",
    "    filtered_text = ' '.join(filtered_words)\n",
    "    \n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 6. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use your data from the acquire to produce a dataframe of the news articles. Name the dataframe news_df.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Read in new_shorts.csv\n",
    "news_df = pd.read_csv('csv_files/news_shorts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 7.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make another dataframe for the Codeup blog posts. Name the dataframe codeup_df.\n",
    "\n",
    "codeup_df = pd.read_csv('csv_files/codeup_blogs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 8. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each dataframe, produce the following columns:\n",
    "\n",
    "# - title to hold the title\n",
    "# - original to hold the original article/post content\n",
    "# - clean to hold the normalized and tokenized original with the stopwords removed.\n",
    "# - stemmed to hold the stemmed version of the cleaned data.\n",
    "# - lemmatized to hold the lemmatized version of the cleaned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>category</th>\n",
       "      <th>original</th>\n",
       "      <th>clean</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jio Financial Services' Q2 profit jumps 101% Q...</td>\n",
       "      <td>Business</td>\n",
       "      <td>Jio Financial Services posted a net profit of ...</td>\n",
       "      <td>jio financial services posted net profit 668 c...</td>\n",
       "      <td>jio financi servic post net profit 668 crore s...</td>\n",
       "      <td>jio financial service posted net profit 668 cr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  category  \\\n",
       "0  Jio Financial Services' Q2 profit jumps 101% Q...  Business   \n",
       "\n",
       "                                            original  \\\n",
       "0  Jio Financial Services posted a net profit of ...   \n",
       "\n",
       "                                               clean  \\\n",
       "0  jio financial services posted net profit 668 c...   \n",
       "\n",
       "                                             stemmed  \\\n",
       "0  jio financi servic post net profit 668 crore s...   \n",
       "\n",
       "                                          lemmatized  \n",
       "0  jio financial service posted net profit 668 cr...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df = news_df.rename(columns={'content': 'original'})\n",
    "\n",
    "news_df['clean'] = news_df.original.apply(basic_clean).apply(tokenize).apply(remove_stopwords)\n",
    "\n",
    "news_df['stemmed'] = news_df.clean.apply(stem)\n",
    "\n",
    "news_df['lemmatized'] = news_df.clean.apply(lemmatize)\n",
    "\n",
    "news_df = news_df[['title', 'category','original', 'clean', 'stemmed', 'lemmatized']]\n",
    "\n",
    "news_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>original</th>\n",
       "      <th>clean</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Spotlight on APIDA Voices: Celebrating Heritag...</td>\n",
       "      <td>May is traditionally known as Asian American a...</td>\n",
       "      <td>may traditionally known asian american pacific...</td>\n",
       "      <td>may tradit known asian american pacif island a...</td>\n",
       "      <td>may traditionally known asian american pacific...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Spotlight on APIDA Voices: Celebrating Heritag...   \n",
       "\n",
       "                                            original  \\\n",
       "0  May is traditionally known as Asian American a...   \n",
       "\n",
       "                                               clean  \\\n",
       "0  may traditionally known asian american pacific...   \n",
       "\n",
       "                                             stemmed  \\\n",
       "0  may tradit known asian american pacif island a...   \n",
       "\n",
       "                                          lemmatized  \n",
       "0  may traditionally known asian american pacific...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codeup_df = codeup_df.rename(columns={'content': 'original'})\n",
    "\n",
    "codeup_df['clean'] = codeup_df.original.apply(basic_clean).apply(tokenize).apply(remove_stopwords)\n",
    "\n",
    "codeup_df['stemmed'] = codeup_df.clean.apply(stem)\n",
    "\n",
    "codeup_df['lemmatized'] = codeup_df.clean.apply(lemmatize)\n",
    "\n",
    "codeup_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 9. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ask yourself:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### If your corpus is 493KB, would you prefer to use stemmed or lemmatized text?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would almost always prefer to lemmatize, due to higher accuracy. It would definitely make sense here as well, due to the corpus being so small and easily processable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### If your corpus is 25MB, would you prefer to use stemmed or lemmatized text?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although this corpus is slightly larger, I would most likely still lemmatize, due to accruacy. Although, if I were on a time crunch or had limited resources, stemming could make sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If your corpus is 200TB of text and you're charged by the megabyte for your hosted computational resources, would you prefer to use stemmed or lemmatized text?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We would most likely use stemming, unless it proved to be absolutely critical to use lemmatized text."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
